{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Common_Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "@author: Jiaxin Ye\n",
    "@contact: jiaxin-ye@foxmail.com\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "\n",
    "class Common_Model(object):\n",
    "\n",
    "    def __init__(self, save_path: str = '', name: str = 'Not Specified'):\n",
    "        self.model = None\n",
    "        self.trained = False \n",
    "\n",
    "    def train(self, x_train, y_train, x_val, y_val):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def predict(self, samples):\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "\n",
    "    def predict_proba(self, samples):\n",
    "        if not self.trained:\n",
    "            sys.stderr.write(\"No Model.\")\n",
    "            sys.exit(-1)\n",
    "        return self.model.predict_proba(samples)\n",
    "\n",
    "    def save_model(self, model_name: str):\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TIMNET\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "@author: Jiaxin Ye\n",
    "@contact: jiaxin-ye@foxmail.com\n",
    "\"\"\"\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.layers import Activation, Lambda\n",
    "from tensorflow.keras.layers import Conv1D, SpatialDropout1D,add,GlobalAveragePooling1D\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.activations import sigmoid\n",
    "\n",
    "def Temporal_Aware_Block(x, s, i, activation, nb_filters, kernel_size, dropout_rate=0, name=''):\n",
    "\n",
    "    original_x = x\n",
    "    #1.1\n",
    "    conv_1_1 = Conv1D(filters=nb_filters, kernel_size=kernel_size,\n",
    "                  dilation_rate=i, padding='causal')(x)\n",
    "    conv_1_1 = BatchNormalization(trainable=True,axis=-1)(conv_1_1)\n",
    "    conv_1_1 =  Activation(activation)(conv_1_1)\n",
    "    output_1_1 =  SpatialDropout1D(dropout_rate)(conv_1_1)\n",
    "    # 2.1\n",
    "    conv_2_1 = Conv1D(filters=nb_filters, kernel_size=kernel_size,\n",
    "                  dilation_rate=i, padding='causal')(output_1_1)\n",
    "    conv_2_1 = BatchNormalization(trainable=True,axis=-1)(conv_2_1)\n",
    "    conv_2_1 = Activation(activation)(conv_2_1)\n",
    "    output_2_1 =  SpatialDropout1D(dropout_rate)(conv_2_1)\n",
    "    \n",
    "    if original_x.shape[-1] != output_2_1.shape[-1]:\n",
    "        original_x = Conv1D(filters=nb_filters, kernel_size=1, padding='same')(original_x)\n",
    "        \n",
    "    output_2_1 = Lambda(sigmoid)(output_2_1)\n",
    "    F_x = Lambda(lambda x: tf.multiply(x[0], x[1]))([original_x, output_2_1])\n",
    "    return F_x\n",
    "\n",
    "\n",
    "class TIMNET:\n",
    "    def __init__(self,\n",
    "                 nb_filters=64,\n",
    "                 kernel_size=2,\n",
    "                 nb_stacks=1,\n",
    "                 dilations=None,\n",
    "                 activation = \"relu\",\n",
    "                 dropout_rate=0.1,\n",
    "                 return_sequences=True,\n",
    "                 name='TIMNET'):\n",
    "        self.name = name\n",
    "        self.return_sequences = return_sequences\n",
    "        self.activation = activation\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.dilations = dilations\n",
    "        self.nb_stacks = nb_stacks\n",
    "        self.kernel_size = kernel_size\n",
    "        self.nb_filters = nb_filters\n",
    "\n",
    "        self.supports_masking = True\n",
    "        self.mask_value=0.\n",
    "\n",
    "        if not isinstance(nb_filters, int):\n",
    "            raise Exception()\n",
    "\n",
    "    def __call__(self, inputs, mask=None):\n",
    "        if self.dilations is None:\n",
    "            self.dilations = 8\n",
    "        forward = inputs\n",
    "        backward = K.reverse(inputs,axes=1)\n",
    "        \n",
    "        print(\"Input Shape=\",inputs.shape)\n",
    "        forward_convd = Conv1D(filters=self.nb_filters,kernel_size=1, dilation_rate=1, padding='causal')(forward)\n",
    "        backward_convd = Conv1D(filters=self.nb_filters,kernel_size=1, dilation_rate=1, padding='causal')(backward)\n",
    "        \n",
    "        final_skip_connection = []\n",
    "        \n",
    "        skip_out_forward = forward_convd\n",
    "        skip_out_backward = backward_convd\n",
    "        \n",
    "        for s in range(self.nb_stacks):\n",
    "            for i in [2 ** i for i in range(self.dilations)]:\n",
    "                skip_out_forward = Temporal_Aware_Block(skip_out_forward, s, i, self.activation,\n",
    "                                                        self.nb_filters,\n",
    "                                                        self.kernel_size, \n",
    "                                                        self.dropout_rate,  \n",
    "                                                        name=self.name)\n",
    "                skip_out_backward = Temporal_Aware_Block(skip_out_backward, s, i, self.activation,\n",
    "                                                        self.nb_filters,\n",
    "                                                        self.kernel_size, \n",
    "                                                        self.dropout_rate,  \n",
    "                                                        name=self.name)\n",
    "                \n",
    "                temp_skip = add([skip_out_forward, skip_out_backward],name = \"biadd_\"+str(i))\n",
    "                temp_skip=GlobalAveragePooling1D()(temp_skip)\n",
    "                temp_skip=tf.expand_dims(temp_skip, axis=1)\n",
    "                final_skip_connection.append(temp_skip)\n",
    "\n",
    "        output_2 = final_skip_connection[0]\n",
    "        for i,item in enumerate(final_skip_connection):\n",
    "            if i==0:\n",
    "                continue\n",
    "            output_2 = K.concatenate([output_2,item],axis=-2)\n",
    "        x = output_2\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMNET_MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "@author: Jiaxin Ye\n",
    "@contact: jiaxin-ye@foxmail.com\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import tensorflow.keras.backend as K\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import SGD,Adam\n",
    "from tensorflow.keras import callbacks\n",
    "from tensorflow.keras.layers import Layer,Dense,Input\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.metrics import confusion_matrix\n",
    "#from Common_Model import Common_Model\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import datetime\n",
    "import pandas as pd\n",
    "\n",
    "#from TIMNET import TIMNET\n",
    "\n",
    "\n",
    "def smooth_labels(labels, factor=0.1):\n",
    "    # smooth the labels\n",
    "    labels *= (1 - factor)\n",
    "    labels += (factor / labels.shape[1])\n",
    "    return labels\n",
    "\n",
    "class WeightLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(WeightLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.kernel = self.add_weight(name='kernel',\n",
    "                                      shape=(input_shape[1],1),\n",
    "                                      initializer='uniform',\n",
    "                                      trainable=True)  \n",
    "        super(WeightLayer, self).build(input_shape)  \n",
    " \n",
    "    def call(self, x):\n",
    "        tempx = tf.transpose(x,[0,2,1])\n",
    "        x = K.dot(tempx,self.kernel)\n",
    "        x = tf.squeeze(x,axis=-1)\n",
    "        return  x\n",
    " \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0],input_shape[2])\n",
    "    \n",
    "def softmax(x, axis=-1):\n",
    "    ex = K.exp(x - K.max(x, axis=axis, keepdims=True))\n",
    "    return ex/K.sum(ex, axis=axis, keepdims=True)\n",
    "\n",
    "class TIMNET_Model(Common_Model):\n",
    "    def __init__(self, args, input_shape, class_label, **params):\n",
    "        super(TIMNET_Model,self).__init__(**params)\n",
    "        self.args = args\n",
    "        self.data_shape = input_shape\n",
    "        self.num_classes = len(class_label)\n",
    "        self.class_label = class_label\n",
    "        self.matrix = []\n",
    "        self.eva_matrix = []\n",
    "        self.acc = 0\n",
    "        print(\"TIMNET MODEL SHAPE:\",input_shape)\n",
    "    \n",
    "    def create_model(self):\n",
    "        self.inputs=Input(shape = (self.data_shape[0],self.data_shape[1]))\n",
    "        self.multi_decision = TIMNET(nb_filters=self.args.filter_size,\n",
    "                                kernel_size=self.args.kernel_size, \n",
    "                                nb_stacks=self.args.stack_size,\n",
    "                                dilations=self.args.dilation_size,\n",
    "                                dropout_rate=self.args.dropout,\n",
    "                                activation = self.args.activation,\n",
    "                                return_sequences=True, \n",
    "                                name='TIMNET')(self.inputs)\n",
    "\n",
    "        self.decision = WeightLayer()(self.multi_decision)\n",
    "        self.predictions = Dense(self.num_classes, activation='softmax')(self.decision)\n",
    "        self.model = Model(inputs = self.inputs, outputs = self.predictions)\n",
    "        \n",
    "        self.model.compile(loss = \"categorical_crossentropy\",\n",
    "                           optimizer =Adam(learning_rate=self.args.lr, beta_1=self.args.beta1, beta_2=self.args.beta2, epsilon=1e-8),\n",
    "                           metrics = ['accuracy'])\n",
    "        print(\"Temporal create succes!\")\n",
    "        \n",
    "    def train(self, x, y):\n",
    "\n",
    "        filepath = self.args.model_path\n",
    "        resultpath = self.args.result_path\n",
    "\n",
    "        if not os.path.exists(filepath):\n",
    "            os.mkdir(filepath)\n",
    "        if not os.path.exists(resultpath):\n",
    "            os.mkdir(resultpath)\n",
    "\n",
    "        i=1\n",
    "        now = datetime.datetime.now()\n",
    "        now_time = datetime.datetime.strftime(now,'%Y-%m-%d_%H-%M-%S')\n",
    "        kfold = KFold(n_splits=self.args.split_fold, shuffle=True, random_state=self.args.random_seed)\n",
    "        avg_accuracy = 0\n",
    "        avg_loss = 0\n",
    "        for train, test in kfold.split(x, y):\n",
    "            self.create_model()\n",
    "            y[train] = smooth_labels(y[train], 0.1)\n",
    "            folder_address = filepath+self.args.data+\"_\"+str(self.args.random_seed)+\"_\"+now_time\n",
    "            if not os.path.exists(folder_address):\n",
    "                os.mkdir(folder_address)\n",
    "            weight_path=folder_address+'/'+str(self.args.split_fold)+\"-fold_weights_best_\"+str(i)+\".hdf5\"\n",
    "            checkpoint = callbacks.ModelCheckpoint(weight_path, monitor='val_accuracy', verbose=1,save_weights_only=True,save_best_only=True,mode='max')\n",
    "            max_acc = 0\n",
    "            best_eva_list = []\n",
    "            h = self.model.fit(x[train], y[train],validation_data=(x[test],  y[test]),batch_size = self.args.batch_size, epochs = self.args.epoch, verbose=1,callbacks=[checkpoint])\n",
    "            self.model.load_weights(weight_path)\n",
    "            best_eva_list = self.model.evaluate(x[test],  y[test])\n",
    "            avg_loss += best_eva_list[0]\n",
    "            avg_accuracy += best_eva_list[1]\n",
    "            print(str(i)+'_Model evaluation: ', best_eva_list,\"   Now ACC:\",str(round(avg_accuracy*10000)/100/i))\n",
    "            i+=1\n",
    "            y_pred_best = self.model.predict(x[test])\n",
    "            self.matrix.append(confusion_matrix(np.argmax(y[test],axis=1),np.argmax(y_pred_best,axis=1)))\n",
    "            em = classification_report(np.argmax(y[test],axis=1),np.argmax(y_pred_best,axis=1), target_names=self.class_label,output_dict=True)\n",
    "            self.eva_matrix.append(em)\n",
    "            print(classification_report(np.argmax(y[test],axis=1),np.argmax(y_pred_best,axis=1), target_names=self.class_label))\n",
    "\n",
    "        print(\"Average ACC:\",avg_accuracy/self.args.split_fold)\n",
    "        self.acc = avg_accuracy/self.args.split_fold\n",
    "        writer = pd.ExcelWriter(resultpath+self.args.data+'_'+str(self.args.split_fold)+'fold_'+str(round(self.acc*10000)/100)+\"_\"+str(self.args.random_seed)+\"_\"+now_time+'.xlsx')\n",
    "        for i,item in enumerate(self.matrix):\n",
    "            temp = {}\n",
    "            temp[\" \"] = self.class_label\n",
    "            for j,l in enumerate(item):\n",
    "                temp[self.class_label[j]]=item[j]\n",
    "            data1 = pd.DataFrame(temp)\n",
    "            data1.to_excel(writer,sheet_name=str(i), encoding='utf8')\n",
    "\n",
    "            df = pd.DataFrame(self.eva_matrix[i]).transpose()\n",
    "            df.to_excel(writer,sheet_name=str(i)+\"_evaluate\", encoding='utf8')\n",
    "        writer.save()\n",
    "        writer.close()\n",
    "\n",
    "        K.clear_session()\n",
    "        self.matrix = []\n",
    "        self.eva_matrix = []\n",
    "        self.acc = 0\n",
    "        self.trained = True\n",
    "    \n",
    "    def test(self, x, y, path):\n",
    "        i=1\n",
    "        kfold = KFold(n_splits=self.args.split_fold, shuffle=True, random_state=self.args.random_seed)\n",
    "        avg_accuracy = 0\n",
    "        avg_loss = 0\n",
    "        x_feats = []\n",
    "        y_labels = []\n",
    "        for train, test in kfold.split(x, y):\n",
    "            self.create_model()\n",
    "            weight_path=path+'/'+str(self.args.split_fold)+\"-fold_weights_best_\"+str(i)+\".hdf5\"\n",
    "            self.model.fit(x[train], y[train],validation_data=(x[test],  y[test]),batch_size = 64,epochs = 0,verbose=0)\n",
    "            self.model.load_weights(weight_path)#+source_name+'_single_best.hdf5')\n",
    "            best_eva_list = self.model.evaluate(x[test],  y[test])\n",
    "            avg_loss += best_eva_list[0]\n",
    "            avg_accuracy += best_eva_list[1]\n",
    "            print(str(i)+'_Model evaluation: ', best_eva_list,\"   Now ACC:\",str(round(avg_accuracy*10000)/100/i))\n",
    "            i+=1\n",
    "            y_pred_best = self.model.predict(x[test])\n",
    "            self.matrix.append(confusion_matrix(np.argmax(y[test],axis=1),np.argmax(y_pred_best,axis=1)))\n",
    "            em = classification_report(np.argmax(y[test],axis=1),np.argmax(y_pred_best,axis=1), target_names=self.class_label,output_dict=True)\n",
    "            self.eva_matrix.append(em)\n",
    "            print(classification_report(np.argmax(y[test],axis=1),np.argmax(y_pred_best,axis=1), target_names=self.class_label))\n",
    "            caps_layer_model = Model(inputs=self.model.input,\n",
    "            outputs=self.model.get_layer(index=-2).output)\n",
    "            feature_source = caps_layer_model.predict(x[test])\n",
    "            x_feats.append(feature_source)\n",
    "            y_labels.append(y[test])\n",
    "        print(\"Average ACC:\",avg_accuracy/self.args.split_fold)\n",
    "        self.acc = avg_accuracy/self.args.split_fold\n",
    "        return x_feats, y_labels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IEMOCAP\n",
      "TIMNET MODEL SHAPE: (606, 39)\n",
      "Input Shape= (None, 606, 39)\n",
      "Temporal create succes!\n",
      "18/18 [==============================] - 3s 105ms/step - loss: 0.2038 - accuracy: 0.9477\n",
      "1_Model evaluation:  [0.2038072645664215, 0.9476534128189087]    Now ACC: 94.77\n",
      "18/18 [==============================] - 3s 103ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       angry       0.96      0.95      0.96       107\n",
      "       happy       0.99      0.90      0.94       163\n",
      "     neutral       0.91      0.98      0.94       171\n",
      "         sad       0.95      0.97      0.96       113\n",
      "\n",
      "    accuracy                           0.95       554\n",
      "   macro avg       0.95      0.95      0.95       554\n",
      "weighted avg       0.95      0.95      0.95       554\n",
      "\n",
      "18/18 [==============================] - 3s 103ms/step\n",
      "Input Shape= (None, 606, 39)\n",
      "Temporal create succes!\n",
      "18/18 [==============================] - 3s 100ms/step - loss: 0.1710 - accuracy: 0.9747\n",
      "2_Model evaluation:  [0.17103344202041626, 0.9746835231781006]    Now ACC: 96.115\n",
      "18/18 [==============================] - 3s 104ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       angry       0.97      0.99      0.98        89\n",
      "       happy       1.00      0.96      0.98       190\n",
      "     neutral       0.97      0.98      0.98       157\n",
      "         sad       0.94      0.98      0.96       117\n",
      "\n",
      "    accuracy                           0.97       553\n",
      "   macro avg       0.97      0.98      0.97       553\n",
      "weighted avg       0.98      0.97      0.97       553\n",
      "\n",
      "18/18 [==============================] - 3s 102ms/step\n",
      "Input Shape= (None, 606, 39)\n",
      "Temporal create succes!\n",
      "18/18 [==============================] - 3s 98ms/step - loss: 0.2386 - accuracy: 0.9675\n",
      "3_Model evaluation:  [0.23863327503204346, 0.9674502611160278]    Now ACC: 96.32666666666667\n",
      "18/18 [==============================] - 3s 105ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       angry       0.98      0.96      0.97       135\n",
      "       happy       0.99      0.97      0.98       151\n",
      "     neutral       0.96      0.96      0.96       159\n",
      "         sad       0.95      0.98      0.96       108\n",
      "\n",
      "    accuracy                           0.97       553\n",
      "   macro avg       0.97      0.97      0.97       553\n",
      "weighted avg       0.97      0.97      0.97       553\n",
      "\n",
      "18/18 [==============================] - 3s 104ms/step\n",
      "Input Shape= (None, 606, 39)\n",
      "Temporal create succes!\n",
      "18/18 [==============================] - 3s 102ms/step - loss: 0.2827 - accuracy: 0.9656\n",
      "4_Model evaluation:  [0.282741516828537, 0.965641975402832]    Now ACC: 96.385\n",
      "18/18 [==============================] - 3s 108ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       angry       0.98      0.94      0.96       116\n",
      "       happy       0.97      0.97      0.97       149\n",
      "     neutral       0.96      0.96      0.96       181\n",
      "         sad       0.95      0.99      0.97       107\n",
      "\n",
      "    accuracy                           0.97       553\n",
      "   macro avg       0.97      0.97      0.97       553\n",
      "weighted avg       0.97      0.97      0.97       553\n",
      "\n",
      "18/18 [==============================] - 3s 101ms/step\n",
      "Input Shape= (None, 606, 39)\n",
      "Temporal create succes!\n",
      "18/18 [==============================] - 3s 98ms/step - loss: 0.4091 - accuracy: 0.9512\n",
      "5_Model evaluation:  [0.40910419821739197, 0.9511753916740417]    Now ACC: 96.132\n",
      "18/18 [==============================] - 2s 101ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       angry       0.97      0.98      0.97        95\n",
      "       happy       0.96      0.93      0.94       166\n",
      "     neutral       0.94      0.95      0.95       180\n",
      "         sad       0.94      0.96      0.95       112\n",
      "\n",
      "    accuracy                           0.95       553\n",
      "   macro avg       0.95      0.96      0.95       553\n",
      "weighted avg       0.95      0.95      0.95       553\n",
      "\n",
      "18/18 [==============================] - 2s 102ms/step\n",
      "Input Shape= (None, 606, 39)\n",
      "Temporal create succes!\n",
      "18/18 [==============================] - 3s 100ms/step - loss: 0.5135 - accuracy: 0.9530\n",
      "6_Model evaluation:  [0.5134909152984619, 0.9529837369918823]    Now ACC: 95.99333333333334\n",
      "18/18 [==============================] - 2s 98ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       angry       0.95      0.95      0.95       112\n",
      "       happy       0.98      0.95      0.96       171\n",
      "     neutral       0.92      0.98      0.95       174\n",
      "         sad       0.96      0.93      0.94        96\n",
      "\n",
      "    accuracy                           0.95       553\n",
      "   macro avg       0.95      0.95      0.95       553\n",
      "weighted avg       0.95      0.95      0.95       553\n",
      "\n",
      "18/18 [==============================] - 2s 99ms/step\n",
      "Input Shape= (None, 606, 39)\n",
      "Temporal create succes!\n",
      "18/18 [==============================] - 3s 98ms/step - loss: 0.6354 - accuracy: 0.8734\n",
      "7_Model evaluation:  [0.6353669166564941, 0.8734177350997925]    Now ACC: 94.75714285714285\n",
      "18/18 [==============================] - 2s 97ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       angry       1.00      0.85      0.92       121\n",
      "       happy       0.86      0.87      0.87       168\n",
      "     neutral       0.86      0.92      0.89       175\n",
      "         sad       0.78      0.82      0.80        89\n",
      "\n",
      "    accuracy                           0.87       553\n",
      "   macro avg       0.88      0.87      0.87       553\n",
      "weighted avg       0.88      0.87      0.87       553\n",
      "\n",
      "18/18 [==============================] - 2s 100ms/step\n",
      "Input Shape= (None, 606, 39)\n",
      "Temporal create succes!\n",
      "18/18 [==============================] - 3s 97ms/step - loss: 0.6457 - accuracy: 0.9078\n",
      "8_Model evaluation:  [0.6456651091575623, 0.9077757596969604]    Now ACC: 94.26\n",
      "18/18 [==============================] - 2s 98ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       angry       0.97      0.91      0.94       117\n",
      "       happy       0.90      0.91      0.90       160\n",
      "     neutral       0.91      0.91      0.91       161\n",
      "         sad       0.86      0.90      0.88       115\n",
      "\n",
      "    accuracy                           0.91       553\n",
      "   macro avg       0.91      0.91      0.91       553\n",
      "weighted avg       0.91      0.91      0.91       553\n",
      "\n",
      "18/18 [==============================] - 2s 100ms/step\n",
      "Input Shape= (None, 606, 39)\n",
      "Temporal create succes!\n",
      "18/18 [==============================] - 3s 99ms/step - loss: 0.5769 - accuracy: 0.9620\n",
      "9_Model evaluation:  [0.5768601298332214, 0.9620253443717957]    Now ACC: 94.47555555555556\n",
      "18/18 [==============================] - 3s 102ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       angry       0.98      0.96      0.97       113\n",
      "       happy       0.95      0.97      0.96       165\n",
      "     neutral       0.95      0.98      0.96       166\n",
      "         sad       0.98      0.93      0.95       109\n",
      "\n",
      "    accuracy                           0.96       553\n",
      "   macro avg       0.97      0.96      0.96       553\n",
      "weighted avg       0.96      0.96      0.96       553\n",
      "\n",
      "18/18 [==============================] - 2s 101ms/step\n",
      "Input Shape= (None, 606, 39)\n",
      "Temporal create succes!\n",
      "18/18 [==============================] - 3s 105ms/step - loss: 0.7060 - accuracy: 0.9277\n",
      "10_Model evaluation:  [0.7060351371765137, 0.9276672601699829]    Now ACC: 94.30499999999999\n",
      "18/18 [==============================] - 3s 103ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       angry       0.97      0.90      0.93        98\n",
      "       happy       0.95      0.92      0.94       153\n",
      "     neutral       0.93      0.93      0.93       184\n",
      "         sad       0.86      0.96      0.91       118\n",
      "\n",
      "    accuracy                           0.93       553\n",
      "   macro avg       0.93      0.93      0.93       553\n",
      "weighted avg       0.93      0.93      0.93       553\n",
      "\n",
      "18/18 [==============================] - 3s 115ms/step\n",
      "Average ACC: 0.9430474400520324\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "@author: Jiaxin Ye\n",
    "@contact: jiaxin-ye@foxmail.com\n",
    "\"\"\"\n",
    "# -*- coding:UTF-8 -*-\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "#from Model import TIMNET_Model\n",
    "import argparse\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument('--mode', type=str, default=\"test\")\n",
    "parser.add_argument('--model_path', type=str, default='./Models/')\n",
    "parser.add_argument('--result_path', type=str, default='./Results/')\n",
    "parser.add_argument('--test_path', type=str, default='./Test_Models/EMODB_46')\n",
    "parser.add_argument('--data', type=str, default='EMODB')\n",
    "parser.add_argument('--lr', type=float, default=0.001)\n",
    "parser.add_argument('--beta1', type=float, default=0.93)\n",
    "parser.add_argument('--beta2', type=float, default=0.98)\n",
    "parser.add_argument('--batch_size', type=int, default=64)\n",
    "parser.add_argument('--epoch', type=int, default=60)\n",
    "parser.add_argument('--dropout', type=float, default=0.1)\n",
    "parser.add_argument('--random_seed', type=int, default=46)\n",
    "parser.add_argument('--activation', type=str, default='relu')\n",
    "parser.add_argument('--filter_size', type=int, default=39)\n",
    "parser.add_argument('--dilation_size', type=int, default=8)# If you want to train model on IEMOCAP, you should modify this parameter to 10 due to the long duration of speech signals.\n",
    "parser.add_argument('--kernel_size', type=int, default=2)\n",
    "parser.add_argument('--stack_size', type=int, default=1)\n",
    "parser.add_argument('--split_fold', type=int, default=10)\n",
    "#parser.add_argument('--gpu', type=str, default='0')\n",
    "\n",
    "dataset_paths = {\n",
    "    \"EMODB\": './Test_Models/EMODB_46',\n",
    "    \"CASIA\": './Test_Models/CASIA_32',\n",
    "    \"EMOVO\": './Test_Models/EMOVO_1',\n",
    "    \"IEMOCAP\" : './Test_Models/IEMOCAP_16',\n",
    "    \"RAVDE\" : './Test_Models/RAVDE_46',\n",
    "    \"SAVEE\" : './Test_Models/SAVEE_44',\n",
    "}\n",
    "\n",
    "\n",
    "args = parser.parse_args('--mode test --model_path ./Models/ --result_path ./Results/ --test_path ./Test_Models/IEMOCAP_16 --data IEMOCAP --lr 0.001 --beta1 0.93 --beta2 0.98 --batch_size 64 --epoch 60 --dropout 0.1 --random_seed 46 --activation relu --filter_size 39 --dilation_size 8 --kernel_size 2 --stack_size 1 --split_fold 10'.split())\n",
    "\n",
    "if args.data==\"IEMOCAP\" and args.dilation_size!=10:\n",
    "    args.dilation_size = 10\n",
    "    print(\"IEMOCAP\")\n",
    "    \n",
    "#os.environ['CUDA_VISIBLE_DEVICES'] = args.gpu\n",
    "#gpus = tf.config.experimental.list_physical_devices(device_type='GPU')\n",
    "#config = tf.compat.v1.ConfigProto()\n",
    "#config.gpu_options.allow_growth=True \n",
    "#session = tf.compat.v1.Session(config=config)\n",
    "#print(f\"###gpus:{gpus}\")\n",
    "\n",
    "CLASS_LABELS_finetune = (\"angry\", \"fear\", \"happy\", \"neutral\",\"sad\")\n",
    "CASIA_CLASS_LABELS = (\"angry\", \"fear\", \"happy\", \"neutral\", \"sad\", \"surprise\")#CASIA\n",
    "EMODB_CLASS_LABELS = (\"angry\", \"boredom\", \"disgust\", \"fear\", \"happy\", \"neutral\", \"sad\")#EMODB\n",
    "SAVEE_CLASS_LABELS = (\"angry\",\"disgust\", \"fear\", \"happy\", \"neutral\", \"sad\", \"surprise\")#SAVEE\n",
    "RAVDE_CLASS_LABELS = (\"angry\", \"calm\", \"disgust\", \"fear\", \"happy\", \"neutral\",\"sad\",\"surprise\")#rav\n",
    "IEMOCAP_CLASS_LABELS = (\"angry\", \"happy\", \"neutral\", \"sad\")#iemocap\n",
    "EMOVO_CLASS_LABELS = (\"angry\", \"disgust\", \"fear\", \"happy\",\"neutral\",\"sad\",\"surprise\")#emovo\n",
    "CLASS_LABELS_dict = {\"CASIA\": CASIA_CLASS_LABELS,\n",
    "               \"EMODB\": EMODB_CLASS_LABELS,\n",
    "               \"EMOVO\": EMOVO_CLASS_LABELS,\n",
    "               \"IEMOCAP\": IEMOCAP_CLASS_LABELS,\n",
    "               \"RAVDE\": RAVDE_CLASS_LABELS,\n",
    "               \"SAVEE\": SAVEE_CLASS_LABELS}\n",
    "\n",
    "\n",
    "data = np.load(\"./MFCC/\"+args.data+\".npy\",allow_pickle=True).item()\n",
    "x_source = data[\"x\"]\n",
    "y_source = data[\"y\"]\n",
    "CLASS_LABELS = CLASS_LABELS_dict[args.data]\n",
    "\n",
    "\n",
    "model = TIMNET_Model(args=args, input_shape=x_source.shape[1:], class_label=CLASS_LABELS)\n",
    "if args.mode==\"train\":\n",
    "    model.train(x_source, y_source)\n",
    "elif args.mode==\"test\":\n",
    "    x_feats, y_labels = model.test(x_source, y_source, path=args.test_path)# x_feats and y_labels are test datas for t-sne\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TFenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
